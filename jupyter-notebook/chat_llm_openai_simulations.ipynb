{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d66074f6-9eb2-4039-942e-7232aeeb774e",
   "metadata": {},
   "source": [
    "# OpenAI Simulations for Chat Log Classification\n",
    "\n",
    "This notebook was developed as part of my work as an undergraduate research assistant in the UC Santa Barbara Computer Science Department. The goal of the project is to evaluate whether large language models—specifically OpenAI’s GPT-4o—can consistently classify human-written summaries of chat logs from multiplayer strategic trading games. These summaries were manually written based on annotated transcripts collected during a socioeconomic experiment.\n",
    "\n",
    "The code in this notebook supports the simulation workflow used to test classification reliability. Each simulation involves sending a structured CSV of summaries to GPT-4o, capturing the model’s predicted category labels, and repeating this process many times to assess consistency.\n",
    "\n",
    "This notebook focuses on the simulation setup, output storage, and aggregation steps used to generate results for Models 4 and 6. It was written to make the automation process transparent and reproducible for future analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd771df-00ee-4517-a599-b2d94269af04",
   "metadata": {},
   "source": [
    "## Model 4: Multi-Label Classification (5 Categories)\n",
    "\n",
    "Model 4 was the first prompt design tested. It allowed GPT-4o to assign multiple categories to a single summary. The categories used were:\n",
    "\n",
    "- Coordination\n",
    "- Efficiency\n",
    "- Conflict\n",
    "- Inequality\n",
    "- Unknown\n",
    "\n",
    "The input was a simplified CSV (`id_chat_input.csv`) containing just the summary ID and text. The first 20 simulations were run manually. The remaining 80 were automated using the OpenAI API with `temperature=0.7`.\n",
    "\n",
    "The outputs from all 100 simulations were saved as individual CSV files and later concatenated into a single file: `sum_model_4.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1b3f26-c5e9-446a-85b4-de04b976e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from time import sleep\n",
    "from openai import OpenAI\n",
    "\n",
    "api_key = \"*your api key*\"\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "input_df = pd.read_csv(\"id_chat_input.csv\")\n",
    "\n",
    "# Convert DataFrame to CSV-style string for GPT input\n",
    "csv_input_string = input_df.to_csv(index=False)\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Use the CSV file with summaries. I want you to classify each summary based on the following categories. \n",
    "The classification is not required to be mutually exclusive—each summary can be classified into multiple categories if needed.\n",
    "Categories:\n",
    "Coordination\n",
    "Efficiency\n",
    "Conflict\n",
    "Inequality\n",
    "Unknown (if it does not fit any of the other categories)\n",
    "The output should include binary category columns. No additional text and do not explain what you did. I only want csv output.\n",
    "\"\"\"\n",
    "\n",
    "output_dir = \"/Users/*your path*/chatgpt_outputsM4\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i in range(21, 101):\n",
    "    try:\n",
    "        print(f\"Running simulation {i}...\")\n",
    "\n",
    "        # GPT-4 API call\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"CSV input:\\n{csv_input_string}\"}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        response_text = response.choices[0].message.content\n",
    "\n",
    "        filename = os.path.join(output_dir, f\"model4_{i}.csv\")\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(response_text)\n",
    "\n",
    "        print(f\"Saved: {filename}\")\n",
    "        sleep(1)  # Pause between calls\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during simulation {i}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb0d056-0007-4bf8-947e-6503e9efad16",
   "metadata": {},
   "source": [
    "## Concatenating Model 4 Simulation Outputs\n",
    "\n",
    "Model 4 involved 100 simulations of GPT-4o using a multi-label classification prompt. Each simulation produced a CSV file stored in the `chatgpt_outputsM4/` directory, where each summary could be assigned one or more of five categories: *Coordination*, *Efficiency*, *Conflict*, *Inequality*, and *Unknown*.\n",
    "\n",
    "This script reads all simulation outputs, performs basic file validation, standardizes column names, and appends metadata for the simulation number and model type. It then merges all the individual outputs into a single comprehensive file.\n",
    "\n",
    "The final dataset, saved as `sum_model_4.csv`, contains all model predictions across simulations and is used for analyzing category assignment frequency and measuring classification consistency across repeated GPT-4o runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab78cc95-d79d-464a-92ad-ea8789083437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# Define input and output paths\n",
    "input_folder = 'chatgpt_outputsM4'\n",
    "output_file = 'sum_model_4.csv'\n",
    "\n",
    "def print_file_preview(filepath):\n",
    "    \"\"\"Print the first few lines of a file for debugging\"\"\"\n",
    "    print(f\"\\nPreviewing file: {filepath}\")\n",
    "    with open(filepath, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i < 5:\n",
    "                print(f\"Line {i+1}: {line.strip()}\")\n",
    "            else:\n",
    "                break\n",
    "    print()\n",
    "\n",
    "# Initialize list to store all dataframes\n",
    "dfs = []\n",
    "\n",
    "# Get all relevant CSV files\n",
    "csv_files = sorted([f for f in os.listdir(input_folder) if f.endswith('.csv')])\n",
    "\n",
    "# Process each CSV file\n",
    "for file in csv_files:\n",
    "    match = re.search(r'model4_(\\d+)\\.csv', file)\n",
    "    if match:\n",
    "        simulation_num = match.group(1)\n",
    "        filepath = os.path.join(input_folder, file)\n",
    "        \n",
    "        try:\n",
    "            print(f\"Processing file: {file}\")\n",
    "            print_file_preview(filepath)\n",
    "\n",
    "            # Check structure\n",
    "            with open(filepath, 'r') as f:\n",
    "                reader = csv.reader(f)\n",
    "                header = next(reader)\n",
    "                if len(header) != 7:\n",
    "                    print(f\"Error: Header has {len(header)} fields instead of 7\")\n",
    "                    print(f\"Header: {header}\")\n",
    "                    sys.exit(1)\n",
    "\n",
    "                for i, row in enumerate(reader, start=2):\n",
    "                    if len(row) != 7:\n",
    "                        print(f\"Error: Line {i} has {len(row)} fields instead of 7\")\n",
    "                        print(f\"Line content: {row}\")\n",
    "                        sys.exit(1)\n",
    "\n",
    "            # Read and process file\n",
    "            df = pd.read_csv(filepath)\n",
    "            df.columns = df.columns.str.title()\n",
    "            df = df.rename(columns={\n",
    "                'Id_Chat': 'id_chat',\n",
    "                'Text': 'text',\n",
    "                'Coordination': 'Coordination',\n",
    "                'Efficiency': 'Efficiency',\n",
    "                'Conflict': 'Conflict',\n",
    "                'Inequality': 'Inequality',\n",
    "                'Unknown': 'Unknown'\n",
    "            })\n",
    "\n",
    "            print(f\"Columns found: {list(df.columns)}\")\n",
    "            print(f\"Number of columns: {len(df.columns)}\")\n",
    "            print(f\"Number of rows: {len(df)}\\n\")\n",
    "\n",
    "            df['Model'] = 4\n",
    "            df['Simulation'] = simulation_num\n",
    "\n",
    "            df = df[['id_chat', 'text', 'Coordination', 'Efficiency', 'Conflict',\n",
    "                     'Inequality', 'Unknown', 'Model', 'Simulation']]\n",
    "\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}:\")\n",
    "            print(str(e))\n",
    "            print(\"Skipping this file...\\n\")\n",
    "            continue\n",
    "\n",
    "if dfs:\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    print(f\"Successfully created {output_file} with {len(final_df)} rows from {len(dfs)} input files.\")\n",
    "else:\n",
    "    print(f\"No valid CSV files found in {input_folder}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271562d1-dd7b-41c7-85b4-9a5b326e17f1",
   "metadata": {},
   "source": [
    "## Model 6: Binary Classification (2 Categories)\n",
    "\n",
    "Model 6 was the final and most simplified prompt design tested. It restricted GPT-4o to assign exactly one of two mutually exclusive categories per summary:\n",
    "\n",
    "- Coordination\n",
    "- Unknown\n",
    "\n",
    "This design was motivated by results from Model 5, where over 95% of outputs already fell into these two categories. By narrowing the classification scope, the goal was to increase output consistency and reduce ambiguity.\n",
    "\n",
    "The input remained the same: a simplified CSV (`id_chat_input.csv`) containing just the summary ID and text. As with previous models, the first 20 simulations were run manually, and the remaining 80 were automated using the OpenAI API with `temperature=0.7`.\n",
    "\n",
    "All 100 outputs were saved as individual CSV files and later concatenated into a single dataset: `sum_model_6.csv`, which was then aggregated to produce `totals_model6.csv` for consistency analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbb38ce-40f3-4c97-976a-05bc549c1fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from time import sleep\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "api_key = \"*your api key*\"\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "input_df = pd.read_csv(\"id_chat_input.csv\")\n",
    "\n",
    "# Convert DataFrame to CSV-style string for GPT input\n",
    "csv_input_string = input_df.to_csv(index=False)\n",
    "\n",
    "user_prompt = \"\"\"\n",
    "Use the CSV file with summaries. I want you to classify each summary based on the following categories. The classification must be mutually exclusive—each summary can be classified into only one category.\n",
    "Categories:\n",
    "Coordination\n",
    "Unknown (if it does not fit any of the other categories)\n",
    "The output should include binary category columns. I only want csv output. \n",
    "\"\"\"\n",
    "\n",
    "output_dir = \"/Users/*your path*/chatgpt_outputsM6\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i in range(21, 101):\n",
    "    try:\n",
    "        print(f\"Running simulation {i}...\")\n",
    "\n",
    "        # GPT-4 API call\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"CSV input:\\n{csv_input_string}\"}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        response_text = response.choices[0].message.content\n",
    "\n",
    "        filename = os.path.join(output_dir, f\"model6_{i}.csv\")\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(response_text)\n",
    "\n",
    "        print(f\"Saved: {filename}\")\n",
    "        sleep(1)  # Pause between calls\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during simulation {i}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf9591d-3f48-4aa2-a23b-46cf4d62ed3e",
   "metadata": {},
   "source": [
    "## Concatenating Model 6 Simulation Outputs\n",
    "\n",
    "After running 100 simulations of Model 6, each individual output was saved as a separate CSV file in the `chatgpt_outputsM6/` directory. Each file contains classification results from a single GPT-4o run, using the same input summaries but potentially varying outputs due to model randomness.\n",
    "\n",
    "This script reads and validates the structure of each CSV file, checks for formatting errors (e.g., incorrect number of columns), and standardizes column names. It then appends simulation metadata (`Model`, `Simulation`) to each dataframe and concatenates them into a unified dataset.\n",
    "\n",
    "The final output is saved as `sum_model_6.csv`, which consolidates all simulation runs for downstream analysis of classification consistency and category frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e0d39c-911a-4ce0-9265-80b15b8aa534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "input_folder = 'chatgpt_outputsM6'\n",
    "output_file = 'sum_model_6.csv'\n",
    "\n",
    "def print_file_preview(filepath):\n",
    "    \"\"\"Print the first few lines of a file for debugging\"\"\"\n",
    "    print(f\"\\nPreviewing file: {filepath}\")\n",
    "    with open(filepath, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i < 5:  # Print first 5 lines\n",
    "                print(f\"Line {i+1}: {line.strip()}\")\n",
    "            else:\n",
    "                break\n",
    "    print()\n",
    "\n",
    "# Initialize an empty list to store all dataframes\n",
    "dfs = []\n",
    "\n",
    "# Get all CSV files from the input folder\n",
    "csv_files = sorted([f for f in os.listdir(input_folder) if f.endswith('.csv')])\n",
    "\n",
    "# Process each CSV file\n",
    "for file in csv_files:\n",
    "    # Extract simulation number from filename using regex\n",
    "    match = re.search(r'model6_(\\d+)\\.csv', file)\n",
    "    if match:\n",
    "        simulation_num = match.group(1)\n",
    "        filepath = os.path.join(input_folder, file)\n",
    "        \n",
    "        try:\n",
    "            print(f\"Processing file: {file}\")\n",
    "            print_file_preview(filepath)\n",
    "            with open(filepath, 'r') as f:\n",
    "                reader = csv.reader(f)\n",
    "                header = next(reader)\n",
    "                if len(header) != 4:\n",
    "                    print(f\"Error: Header has {len(header)} fields instead of 4\")\n",
    "                    print(f\"Header: {header}\")\n",
    "                    sys.exit(1)\n",
    "                \n",
    "                for i, row in enumerate(reader, start=2):\n",
    "                    if len(row) != 4:\n",
    "                        print(f\"Error: Line {i} has {len(row)} fields instead of 4\")\n",
    "                        print(f\"Line content: {row}\")\n",
    "                        sys.exit(1)\n",
    "            \n",
    "            df = pd.read_csv(filepath)    \n",
    "            df.columns = df.columns.str.title() \n",
    "            df = df.rename(columns={\n",
    "                'Id_Chat': 'id_chat',\n",
    "                'Text': 'text',\n",
    "                'Coordination': 'Coordination',\n",
    "                'Unknown': 'Unknown'\n",
    "            })\n",
    "            \n",
    "            print(f\"Columns found: {list(df.columns)}\")\n",
    "            print(f\"Number of columns: {len(df.columns)}\")\n",
    "            print(f\"Number of rows: {len(df)}\\n\")\n",
    "            \n",
    "            df['Model'] = 6\n",
    "            df['Simulation'] = simulation_num\n",
    "            df = df[['id_chat', 'text', 'Coordination', 'Unknown', 'Model', 'Simulation']]\n",
    "            dfs.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file}:\")\n",
    "            print(str(e))\n",
    "            print(\"Skipping this file...\\n\")\n",
    "            continue\n",
    "\n",
    "if dfs:\n",
    "    final_df = pd.concat(dfs, ignore_index=True)\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    print(f\"Successfully created {output_file} with {len(final_df)} rows from {len(dfs)} input files.\")\n",
    "else:\n",
    "    print(f\"No CSV files matching the pattern 'model6_*.csv' were found in {input_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b868640-0719-4746-964d-0720dc1ea3c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Notebook created by:** Anna Gornyitzki  \n",
    "**Date:** July 2025  \n",
    "**Affiliation:** Undergraduate Research Assistant, Computer Science, UC Santa Barbara  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
